{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_W1rI742nZm"
      },
      "source": [
        "# **ReadMe:**\n",
        "\n",
        "Note: The numbering below indicates the cell number and are written in a sequential manner.\n",
        "\n",
        "1) First of all, we need to upload the data set by uploading the respective Excel sheets from the local drive, so for that please run the first cell and upload those files.\n",
        "\n",
        "---\n",
        "2) In the second section, the feature matrix and the output matrix is generated. So run it, as it will be used later for running the algorithm.\n",
        "\n",
        "---\n",
        "3) Run it to see the Descriptive Analysis of the Data Set.\n",
        "\n",
        "---\n",
        "4) Visual Analysis of the data is performed, once you run the code the result will be a set of Histogram plots.\n",
        "\n",
        "---\n",
        "5) 5th Cell will show the Descriptive Analysis of the Credit Card Data set.\n",
        "\n",
        "---\n",
        "6) 6th Cell will be used to run the Visual Analysis of Credit Card Dataset.\n",
        "\n",
        "---\n",
        "7) This function will drop a particular feature from the data set and return the new feature matrix.\n",
        "\n",
        "---\n",
        "8) This function is used for increasing the order of the feature and adding those to the feature matrix. Basically, it is used to make the model more complex by adding the polynomial terms such as cube or square of the feature(s). \n",
        "\n",
        "---\n",
        "9) This is the Class for Logistic Regression, and need to be run to perform the Model Training/Learning using Logistic Regression.\n",
        "\n",
        "---\n",
        "10) This includes the function which calcualtes accuracy of the Model by comparing one vector to the Other Vector.\n",
        "\n",
        "---\n",
        "11) This includes the functions which implement k-fold validation on the model. \n",
        "\n",
        "---\n",
        "12) This cell is used to generate the feature matrix for different models. We can add more models to this, currently 5 models are written for Orthopedic Patients and Credit Card problem.\n",
        "\n",
        "---\n",
        "13) This is the most important part of the code for k-fold implementation. Few lines are commented which can be uncommented to see the results. Also, in the last few lines the X_op_Tn and X_cc_Tn; where n varies from 1 to 5 for each models.\n",
        "\n",
        "---\n",
        "14) This part is used for generating plots depicting Accuracy measurement with respect to alpha (learning rate) and the number of iterations. Last few lines are commented and can be uncommented to see the results. Also, the first argument in the function call can be changed accordingly to requirement of generating a plot for the model chosen by the user.\n",
        "\n",
        "---\n",
        "15) Parameters varies in this section according to the experiment wanted to perform. The 6th Model for the Credit Card Data Set is performed here where in total 11 features were removed.\n",
        "\n",
        "---\n",
        "16) This is used to generate the Confusion Matrix for the data set after training.\n",
        "\n",
        "---\n",
        "17) Logistic Regression is performed using the Inbuilt Python Library SciKit-Learn.\n",
        "\n",
        "---\n",
        "18) Naive Bayes Classifier is peformed using the inbuilt Python Scikit-Learn Library.\n",
        "\n",
        "---\n",
        "19) The scatter plot for both the data set is performed in the Last Cell. Run to see th Scatter Plot.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOjh4M9JFsmv"
      },
      "source": [
        "This is the main python file for Assignment-1 of ECSE-551: Machine Learning for Engineers\n",
        "- In this code, Logistic Regression is implemented on Credit Card and\n",
        "  Orthopedic Patients Datasets.\n",
        "\n",
        "Project made by:     \n",
        "1. Aishwarya Ramamurthy (ID: 260963956)\n",
        "2. Alok Patel (ID: 260954024)\n",
        "3. Katyayani Prakash (ID: 260964511)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-sgaNqyP-B0"
      },
      "source": [
        "# Logistic Regression\n",
        "- Applied on the Credit Card and Orthopedic Patient Data Set\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPcsEZOVFkm2"
      },
      "source": [
        "**Loading the data from the Local Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pNLbLhlsiSOa"
      },
      "source": [
        "from google.colab import files \n",
        "import numpy as np\n",
        "import io\n",
        "import pandas as pd \n",
        " \n",
        "print(\"Please upload creditcard.csv file saved on your local drive\")\n",
        "ccfile = files.upload()                                                         #Uploading creditcard file\n",
        "df_cc = pd.read_csv(io.BytesIO(ccfile['creditcard.csv']))                       #dataframe that stores creditcard.csv\n",
        "#print(df_cc)\n",
        " \n",
        "print(\"Please upload orthopedic_patients.csv file saved on your local drive\")\n",
        "opfile = files.upload()                                                         #Uploading orthopedic patients file\n",
        "df_op = pd.read_csv(io.BytesIO(opfile['orthopedic_patients.csv']))              #dataframe that stores orthopedic_patients.csv\n",
        "#print(df_op)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2A7-kgX8d9B"
      },
      "source": [
        "**Creating the X and y matrices from dataframes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkiUB5n2iSTz"
      },
      "source": [
        "array1 = df_cc.values\n",
        "array2 = df_op.values\n",
        "\n",
        "X_cc = array1[:,0:29]                                                           #Extracting feature from the Credit Card data set\n",
        "Y_cc = np.vstack(array1[:,29])                                                  #Output vector for the credit card data set\n",
        "X_op = array2[:,0:6]                                                            #Extracting feature from the Orthopedic data set\n",
        "Y_op = np.vstack(array2[:,6])                                                   #Output vector for the Orthopedic patient data set\n",
        "\n",
        "#Adding the dummy feature 1 to the end of both training sets.\n",
        "X0 = np.ones((991,1))\n",
        "X_cc = np.hstack((X_cc,X0))       \n",
        "\n",
        "X01 = np.ones((310,1))\n",
        "X_op = np.hstack((X_op,X01))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WqruPAE8icL"
      },
      "source": [
        "**Statistical analysis of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQN83-UriSXq"
      },
      "source": [
        "#Descriptive Analysis of Orthopedic Patients Dataset\n",
        "from pandas import set_option\n",
        "\n",
        "shape = df_op.shape\n",
        "print(\"Shape of Op training set = \",shape)\n",
        "\n",
        "set_option('display.max_columns', None)\n",
        "set_option('precision', 2)\n",
        "\n",
        "description = df_op.describe()\n",
        "print(\"\\nData Description :\\n\",description)\n",
        "\n",
        "class_counts = df_op.groupby('Class').size()\n",
        "print(\"\\nClass Distribution of Training Set\\n\",class_counts)\n",
        "\n",
        "correlations = df_op.corr(method='pearson')                                     #A correlation of -1 or 1 shows a full negative or positive correlation among the features respectively. \n",
        "print(\"\\nCorrelation Matrix of Op\\n\",correlations)                              #A value of 0 shows no correlation at all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZVLZ3h8nQr"
      },
      "source": [
        "**Visual Analysis of Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqND5WgTiSba"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy\n",
        "\n",
        "#Visual Analysis of Orthopedic Patients Dataset\n",
        "\n",
        "#Histogram of all features\n",
        "df_op.hist(figsize=[14,10])\n",
        "pyplot.show()\n",
        " \n",
        "#Density graph of all features\n",
        "df_op.plot(kind='density', subplots=True, layout=(3,3), sharex=False,figsize=[14,10])\n",
        "pyplot.show()\n",
        " \n",
        "#Correlation matrix \n",
        "names = ['pelvic_incidence','pelvic_tilt numeric','lumbar_lordosis_angle','sacral_slope','pelvic_radius','degree_spondylolisthesis','Class']\n",
        "fig = pyplot.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
        "fig.colorbar(cax)\n",
        "ticks = numpy.arange(0,7,1)\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_yticks(ticks)\n",
        "ax.set_xticklabels(names)\n",
        "ax.set_yticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_80ds5di8sZo"
      },
      "source": [
        "**Descriptive Analysis of Creditcard Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EfnTBcfiSei"
      },
      "source": [
        "from pandas import set_option\n",
        "\n",
        "shape = df_cc.shape\n",
        "print(\"Shape of Creditcard training set = \",shape)\n",
        "\n",
        "class_counts = df_cc.groupby('Unnamed: 29').size()\n",
        "print(\"\\nClass Distribution of Training Set\\n\",class_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GffAZkc81A6"
      },
      "source": [
        "**Visual Analysis of Creditcard Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NnFLVQZiShZ"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "\n",
        "mean_cc = df_cc.mean()\n",
        "#print(mean_cc)\n",
        "#mean_cc.hist()\n",
        "#pyplot.show()\n",
        "#mean_cc.plot(kind='density', subplots=True, layout=(3,3), sharex=False,figsize=[14,10])\n",
        "\n",
        "std_cc = df_cc.std()\n",
        "print(std_cc)\n",
        "std_cc.hist()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a54QPq_T9AfL"
      },
      "source": [
        "**Feature selection of training sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WUNaGrW3zti"
      },
      "source": [
        "#This function drops one selected feature from our dataset\n",
        "def featureDrop(X,col):\n",
        "  X_dropped = np.delete(X, col, axis=1)\n",
        "  return X_dropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0dpugLq38uV"
      },
      "source": [
        "#This function takes the array, its column, and order as inputs and returns an array which has the new feature appended as the last column.\n",
        "def polyModel(X,index,order):\n",
        "  n, m = X.shape\n",
        "  new_col = np.zeros((n,1))\n",
        "  for i in range(n):\n",
        "    new_col[i][0] = pow(X[i][index],order)\n",
        "  X_pow = np.append(X,new_col,axis = 1)                                         #Adding new polynomial feature to the existing feature matrix\n",
        "  return X_pow\n",
        "\n",
        "#Calling the function for orthopedic patients dataset\n",
        "X_op_sq = polyModel(X_op,5,2)\n",
        "shape = X_op_sq.shape;\n",
        "shape_old = X_op.shape;\n",
        "print(\"Shape of linear model =\", shape_old)\n",
        "print(\"\\nShape of polynomial model = \",shape)\n",
        "print(\"\\nPolynomial model with powered feature appended on the end =\\n\",X_op_sq)\n",
        "\n",
        "X_op_cub = polyModel(X_op,5,3)\n",
        "print(\"\\nPolynomial model with powered feature appended on the end =\\n\",X_op_cub)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI6N6coQ7p1a"
      },
      "source": [
        "**Class for training and the validation of the Orthopedic Patients and Credit Card Problem**\n",
        "\n",
        "Notes:\n",
        "- The constructor of the class is made using the two Arguments i) X-Feature Matrix and ii) Y-Output Label Vector\n",
        "- Later for the Fit function the arguments are number of epoch and learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Nf0UlI3_z0"
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "class LogisticRegression:\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X \n",
        "    self.y = y \n",
        " \n",
        "  #This function is used to train the model\n",
        "  def fit(self, epoch, alpha):              \n",
        "    n, m = self.X.shape                                                         #Getting the no. of training examples and no. of features\n",
        "    Wk = np.zeros((m,1))                                                        #Initializing the weight vector with zeros\n",
        "    k = 0                                                                                                                       \n",
        "    loss = []\n",
        " \n",
        "    while True: \n",
        "      #if eps < 0.1:                                                            #Epsilon error is taken as 1%  \n",
        "      if k > epoch:                                                             #If the number of iterations k exceeds epoch, the algorithm is terminated.\n",
        "        break\n",
        "\n",
        "      a = np.dot(self.X,Wk)                                                     #Log-odd ratio is calculated\n",
        "      y_hat = self.sigmoid(a)                                                   #Calling the sigmoid function to calculate the probability of the class\n",
        "\n",
        "      dy = (1/n)*(self.y - y_hat)                                               #Difference in the original and estimated target vector\n",
        "      dW = (1/n)*(np.dot(self.X.T,dy))                                          #Change in update in the Weights by minimization\n",
        "      #loss.append(self.costfunction(self.X, self.y, Wk))\n",
        "\n",
        "      Wk1 = Wk + alpha*dW                                                       #Updating the Weights vector\n",
        "      k = k + 1                                                                 #Increasing the iteration count\n",
        "      error = np.square(Wk1 - Wk)                                               #Calculating the difference in weights between two iterations     \t                                                  \n",
        "      #eps = error.max()                                                        #Getting eps as the maximum difference in the weight matrix(vector){Infinity norm}; Required only when epsilon is used for convergence\n",
        "      Wk = Wk1                                                                                                                                               \n",
        " \n",
        "    return Wk1                                                                  #Function returns the updated weight vector\n",
        " \n",
        "  #This function is used to calculate the estimated probability of the class\n",
        "  def sigmoid(self, a):         \n",
        "    return 1/(1+np.exp(-a))\n",
        " \n",
        "  def costfunction(self, X, y, W):        \n",
        "    a = np.dot(X, W)\n",
        "    sig = self.sigmoid(a)\n",
        "    n = len(X)\n",
        "    class_1 = np.multiply(y, np.log(sig))\n",
        "    class_0 = np.multiply((1-y), np.log(1 - sig))\n",
        "    CEL = -np.sum(class_1 + class_0) / n\n",
        "    return CEL\n",
        "\n",
        "  #This function is used to predict our class for given input and weights\n",
        "  def predict(self,X_v, weights):                \n",
        "    pred_vec = np.zeros((len(X_v),1));                                          #Defining a predict vector initialized with zeros       \n",
        "    pred_vec = self.sigmoid(np.dot(X_v,weights))                                #Storing the predicted \"Class\" values into pred_vec\n",
        "    for i in range(len(pred_vec)):                                              #Checking for each output of each training example\n",
        "      if pred_vec[i] >= 0.5:                                                    #It has feature of thresholding the output to 0 and 1.\n",
        "        pred_vec[i] = 1\n",
        "      else:\n",
        "        pred_vec[i] = 0\n",
        "    return pred_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvu4nJhF6oxc"
      },
      "source": [
        "\n",
        "**Cell for the Accuracy Evaluation of the Model**\n",
        "- Function is defined for the Accuracy evalution of the output of the trained model\n",
        "  with the actual output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ8br_Y-4sCa"
      },
      "source": [
        "def Accu_eval(test_y,pred_y):                    \n",
        "  cnt = 0;                                                                      #Count variable\n",
        "  if(len(test_y) == len(pred_y)):                                             \n",
        "    for i in range(len(test_y)):\n",
        "      if(test_y[i] == pred_y[i]):               \n",
        "        cnt += 1                              \n",
        "        acc_score = cnt / len(test_y)                                           #Accuracy score = Total count/Length of test data set\n",
        "    print(\"Accuracy Score = \", acc_score*100,\"%\")                               #Printing Accuracy score in percentage \n",
        "    m=np.zeros((len(test_y,),2)).astype(int);                                   #Initializing Matrix 'm' with zeros that stores the actual target and predicted target column wise\n",
        "    m[:,0]=np.reshape(test_y,(len(test_y,)));                                   #Storing the actual target vector into the 1st column\n",
        "    m[:,1]=np.reshape(pred_y,(len(pred_y,)));                                   #Storing the predicted target vector into the 2nd column\n",
        "  \n",
        "  else:\n",
        "        print(\"Dimension mismatch of Test and Predicted data set\")\n",
        "  return acc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OacDJ-aw7G4O"
      },
      "source": [
        "**k-fold cross validation implementation using Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OnzhSww4vh7"
      },
      "source": [
        "# kfold divides the data into validation and the training set\n",
        "def kfold(X, Y, l):     \n",
        "  X_t, Y_t = kfoldtrain(X,Y,l)\n",
        "  X_v, Y_v = kfoldvalidate(X,Y,l)\n",
        "  return X_t, X_v, Y_t, Y_v\n",
        "\n",
        "def ktrain(X, y, l):                                                            #Feature for training of the model\n",
        "  X = np.delete(X,l,0)\n",
        "  y = np.delete(y,l,0)\n",
        "  return X, y\n",
        "\n",
        "def kvalidate(X, y, l):                                                         #Output Vector for the validation of the model\n",
        "  X = X[l,:]\n",
        "  y = y[l,:]\n",
        "  return X, y\n",
        "\n",
        "#Function used to perform the k-fold\n",
        "def kfold_All(X, Y, Alpha, Epoch, l):\n",
        "  print(\"# Accuracy measuremnt for alpha = \"+ str(Alpha) +\", with Epoch = \"+ str(Epoch))\n",
        "  avg1 = 0\n",
        "  avg2 = 0\n",
        "  for i in range(len(l)):                                                       #Loop do the K-fold validation\n",
        "    X_t, Y_t = ktrain(X,Y,l[i])        \n",
        "    X_v, Y_v = kvalidate(X,Y,l[i]) \n",
        "    model = LogisticRegression(X_t,Y_t)                                         #Making Object of the Class Logistic Regression\n",
        "    weight = model.fit(Epoch, Alpha)                                            #Learning the model\n",
        "    test_y = model.predict(X_v, weight)                                         #Output prediction of the model\n",
        "    #print(test_y)       \n",
        "    print(\"\\nFor \"+str(i+1)+\"th fold.\")\n",
        "    print(\"%% Validation accuracy %%\")            \n",
        "    acc_val = Accu_eval(test_y, Y_v)                                            #Validation Accuracy Calculation\n",
        "    avg1 = acc_val + avg1\n",
        "    #print(weight)\n",
        "    test_y = model.predict(X_t, weight) \n",
        "    print(\"%% Training accuracy %%\")\n",
        "    acc_trai = Accu_eval(test_y, Y_t)                                           #Training Accuracy Calculation\n",
        "    avg2 = acc_trai + avg2\n",
        "\n",
        "  print(\"\\nAverage Accuracy of Validation Set is : \"+str((avg1/len(l))*100))\n",
        "  print(\"\\nAverage Accuracy of Training Set is : \"+str((avg2/len(l))*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A60KiEQ7Q8A"
      },
      "source": [
        "**Creation of Feature Matrix for T-Models**\n",
        "- This is used for the k-fold experimentation later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBp0baHZ4yeq"
      },
      "source": [
        "# Feature Matrix for T models. \n",
        "\n",
        "# Model-1\n",
        "X_op_T1 = X_op\n",
        "X_cc_T1 = X_cc\n",
        "\n",
        "# Model-2 \n",
        "X_op_T2 = polyModel(X_op,1,2)\n",
        "X_cc_T2 = featureDrop(X_cc,28)\n",
        "\n",
        "# Model-3\n",
        "X_op_T3 = polyModel(X_op,5,2)\n",
        "X_cc_T3 = featureDrop(X_cc,28)\n",
        "X_cc_T3 = polyModel(X_cc_T3,2,3)\n",
        "\n",
        "# Model-4\n",
        "X_op_T4 = polyModel(X_op,1,3)\n",
        "X_cc_T4 = featureDrop(X_cc,28)\n",
        "X_cc_T4 = polyModel(X_cc_T3,11,2)\n",
        "\n",
        "# Model-5\n",
        "X_op_T5 = polyModel(X_op,5,3)\n",
        "X_cc_T5 = featureDrop(X_cc,28)\n",
        "X_cc_T5 = polyModel(X_cc_T3,11,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXJHullZ_WlT"
      },
      "source": [
        "**Running the 10 fold cross validation for both the data set for different Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLpsSJHm41xX"
      },
      "source": [
        "#This range is for 10-folds of each Data Set.\n",
        "\n",
        "#Orthopedic Patient problem.\n",
        "op_k = [[i for i in range(0,31)],[i for i in range(31,62)],[i for i in range(62,93)],\n",
        "        [i for i in range(93,124)],[i for i in range(124,155)],[i for i in range(155,186)],\n",
        "        [i for i in range(186,217)],[i for i in range(217,248)],[i for i in range(248,279)],[i for i in range(279,310)]]\n",
        "\n",
        "#Credit Card problem.\n",
        "cc_k = [[i for i in range(0,99)],[i for i in range(99,198)],[i for i in range(198,297)],       \n",
        "        [i for i in range(297,396)],[i for i in range(396,495)],[i for i in range(495,594)],\n",
        "        [i for i in range(594,693)],[i for i in range(693,792)],[i for i in range(792,891)],[i for i in range(891,991)]]\n",
        "\n",
        "#fold_10 = kFoldCrossValidation()                                               #When Class of K-fold was made it was used.\n",
        "\n",
        "#Running k-fold for whole Credit Card Data set.\n",
        "kfold_All(X_cc, Y_cc, 0.15, 1000,cc_k)\n",
        "\n",
        "#Running k-fold for whole Orthopedic Patinet Data Set.\n",
        "kfold_All(X_op, Y_op, 0.28, 500,op_k)\n",
        "\n",
        "#Running k-fold for different T Models.[First op and second cc]\n",
        "#kfold_All(X_op_T5, Y_op, 1, 500, op_k)\n",
        "kfold_All(X_cc_T3, Y_cc, 0.15, 1000,cc_k)\n",
        "\n",
        "#K-fold for the 6th model of the Credi Card case where the 11 features are removed. \n",
        "#kfold_All(X_cc_final, Y_cc, 0.15, 1000,cc_k)                                   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fydRVRlM9Q7Z"
      },
      "source": [
        "**Model Testing with different Epochs(No. of Iterations) and Learning rates**\n",
        "- In the second For loop the second argument is variable according to the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWgAfQt95PXO"
      },
      "source": [
        "# Input parameters (in this order): model_exp(Features,Target,Learning Rate (lower limit),Learning Rate (Upper limit),Step Size,No. of Iterations)\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "def model_exp(X,Y,alpha_range_lowlt,alpha_range_uplt,stp_size,itr):             #For the Testing of the Model\n",
        "    lea_rate=np.arange(alpha_range_lowlt,alpha_range_uplt,stp_size)                                   \n",
        "    acc_vec=np.zeros(len(lea_rate))\n",
        "    t_1 = np.zeros(len(lea_rate))\n",
        "    t_2 = np.zeros(itr)\n",
        "    acc_vec1=np.zeros(itr)\n",
        "    model1 = LogisticRegression(X,Y) \n",
        "    iteration = []\n",
        "\n",
        "    for i in range(len(lea_rate)):                                              #To get the accuracy w.r.t. varying learning rate \n",
        "      t1 = time.time()                                  \n",
        "      weights = model1.fit(itr,lea_rate[i])\n",
        "      t2 = time.time()\n",
        "      test_y1 = model1.predict(X, weights) \n",
        "      acc_vec[i] = np.round((Accu_eval(test_y1, Y)*100),3)  \n",
        "      t_1[i] = t2 - t1\n",
        "\n",
        "#In the fit function here, 0.15 is used for the CC and 0.28 for the OP.\n",
        "    for i in range(itr):                                                        #To get the accuracy w.r.t. iteration \n",
        "      t3 = time.time()\n",
        "      weights1 = model1.fit(i+1,0.15)                                           #Second argument will be changed accoring to the data set. \n",
        "      t4 = time.time()                         \n",
        "      test_y11 = model1.predict(X, weights1) \n",
        "      acc_vec1[i] = np.round((Accu_eval(test_y11, Y)*100),2)\n",
        "      iteration = iteration + [i]\n",
        "      t_2[i] = t4 - t3\n",
        "\n",
        "    #print(\"Learning Rates:\\n\",np.reshape(lea_rate,(len(lea_rate),1)),\"\\nAccuracy(%):\\n\",np.reshape(acc_vec,(len(acc_vec),1)))\n",
        "    plt.plot(lea_rate,acc_vec); \n",
        "    plt.xlabel('Learning Rate (alpha)'); plt.ylabel('Accuracy(%)') \n",
        "    plt.title('Learning Rate vs Accuracy') \n",
        "    plt.show() \n",
        "    plt.plot(lea_rate,t_1)\n",
        "    plt.xlabel('Learning Rate (alpha)'); plt.ylabel('Time(s)') \n",
        "    plt.title('Learning Rate vs Time')\n",
        "    plt.show()\n",
        "    plt.plot(iteration,acc_vec1)\n",
        "    plt.xlabel('Iterations (Epoch)'); plt.ylabel('Accuracy(%)') \n",
        "    plt.title('Iterations vs Accuracy')\n",
        "    plt.show()\n",
        "    plt.plot(iteration,t_2)\n",
        "    plt.xlabel('Iterations (Epoch)'); plt.ylabel('Time(s)') \n",
        "    plt.title('Iterations vs Time')\n",
        "    plt.show()\n",
        "    ind = np.where(acc_vec == max(acc_vec)) \n",
        "    print(\"Maximum Accuracy:\",max(acc_vec),\"%\",\"for Learning Rate:\",list(lea_rate[ind].round(3)))\n",
        "\n",
        "#Running the Model\n",
        "model_exp(X_cc,Y_cc,0,1,0.01,1000)\n",
        "#model_exp(X_op,Y_op,0,1,0.001,500)\n",
        "#model_exp(X_cc_T2,Y_cc,0,1,0.001,1000)\n",
        "#model_exp(X_op_T2,Y_op,0,0.35,0.001,500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjYqJ0KO9gkE"
      },
      "source": [
        "**File to train and Validate for various Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkcZFvsu5TTM"
      },
      "source": [
        "#This is used for random experiment of the different models with different parameters.\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy import set_printoptions\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "#This function is used to evaluate accuracy of our predicted output (y_hat) with true output (y)\n",
        "def Accu_eval(test_y,pred_y):                    \n",
        "  cnt = 0;                                                                        #Count variable\n",
        "  if(len(test_y) == len(pred_y)):                                             \n",
        "    for i in range(len(test_y)):\n",
        "      if(test_y[i] == pred_y[i]):               \n",
        "        cnt += 1                              \n",
        "        acc_score = cnt / len(test_y)                                             #Accuracy score = Total count/Length of test data set\n",
        "    print(\"Accuracy Score = \", acc_score*100,\"%\")                                 #Printing Accuracy score in percentage \n",
        "    m=np.zeros((len(test_y,),2)).astype(int);                                     #Initializing Matrix 'm' with zeros that stores the actual target and predicted target column wise\n",
        "    m[:,0]=np.reshape(test_y,(len(test_y,)));                                     #Storing the actual target vector into the 1st column\n",
        "    m[:,1]=np.reshape(pred_y,(len(pred_y,)));                                     #Storing the predicted target vector into the 2nd column\n",
        "  \n",
        "  else:\n",
        "        print(\"Dimension mismatch of Test and Predicted data set\")\n",
        "  return acc_score\n",
        "\n",
        "#Pre-processing data functions\n",
        "\n",
        "#This functions standardizes the dataset (mean 0, and std deviation 1)\n",
        "def Standardize(X):\n",
        "  scaler = StandardScaler().fit(X)\n",
        "  rescaledX = scaler.transform(X)\n",
        "  # summarize transformed data\n",
        "  #set_printoptions(precision=3))\n",
        "  return rescaledX\n",
        "\n",
        "#This function normalizes the dataset\n",
        "def Normalize(X):\n",
        "  scaler = Normalizer().fit(X)\n",
        "  normalizedX = scaler.transform(X)\n",
        "  # summarize transformed data\n",
        "  #set_printoptions(precision=3)\n",
        "  return normalizedX\n",
        "\n",
        "#Testing the effect of learning rate, feature selection and preprocessing data on our accuracy\n",
        "\n",
        "#Testing the model for epochs = 10 & alpha = 0.15             \n",
        "model1 = LogisticRegression(X_cc,Y_cc)                                    \n",
        "weights = model1.fit(10,0.15)\n",
        "print(\"For epochs = 10 & alpha = 0.15:\")\n",
        "#print(\"The weights of our model are:\\n\",weights)\n",
        "test_y1 = model1.predict(X_cc, weights)                       \n",
        "Accu_eval(test_y1, Y_cc)            \n",
        "#print(\"Predicted value of Y is\\n\",test_y)\n",
        "\n",
        "\n",
        "#Testing the model for epochs = 500 & alpha = 0.2\n",
        "model2 = LogisticRegression(X_cc,Y_cc)                                    \n",
        "weights = model2.fit(500,0.2)\n",
        "print(\"\\nFor epochs = 500 & alpha = 0.2:\")\n",
        "#print(\"The weights of our model are:\\n\",weights)\n",
        "test_y2 = model2.predict(X_cc, weights)                       \n",
        "Accu_eval(test_y2, Y_cc) \n",
        "\n",
        "\n",
        "#Testing the model for epochs = 1000 & alpha = 0.15\n",
        "model3 = LogisticRegression(X_cc,Y_cc)                                    \n",
        "weights = model3.fit(1000,0.15)\n",
        "print(\"\\nFor epochs = 1000 & alpha = 0.15:\")\n",
        "#print(\"The weights of our model are:\\n\",weights)\n",
        "test_y3 = model3.predict(X_cc, weights)                       \n",
        "Accu_eval(test_y3, Y_cc) \n",
        "\n",
        "\n",
        "#Testing the model for epochs = 100 & alpha = 0.3\n",
        "model4 = LogisticRegression(X_cc,Y_cc)                                    \n",
        "weights = model4.fit(100,0.3)\n",
        "print(\"\\nFor epochs = 100 & alpha = 0.3\")\n",
        "#print(\"The weights of our model are:\\n\",weights)\n",
        "test_y4 = model4.predict(X_cc, weights)                       \n",
        "Accu_eval(test_y4, Y_cc)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "#This loop drops one column of features in every recursion till all features have been dropped once\n",
        "n, m = X_cc.shape\n",
        "print(\"n =\",n) \n",
        "print(\"m=\",m)\n",
        "for i in range(m):\n",
        "  X_dropped = featureDrop(X_cc,i)\n",
        "  print(\"Dropping features of column no: \",i)\n",
        "  model_fs = LogisticRegression(X_dropped,Y_cc)\n",
        "  weights = model_fs.fit(1000,0.15)\n",
        "  test_y = model_fs.predict(X_dropped, weights)                       \n",
        "  Accu_eval(test_y, Y_cc)\n",
        "  print(\"\\n\") \n",
        "\n",
        "#Deleting all the extra features from creditcard training set and testing accuracy for that\n",
        "X_cc_final = np.delete(X_cc, [7,12,14,17,18,19,22,23,25,26,27], axis=1)\n",
        "print(\"X_cc after all dropped features is:\",X_cc_final)\n",
        "print(\"Shape of dataset is now :\",X_cc_final.shape)\n",
        "model10 = LogisticRegression(X_cc_final,Y_cc)                                    \n",
        "weights = model10.fit(1000,0.15)\n",
        "print(\"\\nFor epochs = 1000 & alpha = 0.15\")\n",
        "#print(\"The weights of our model are:\\n\",weights)\n",
        "test_y10 = model10.predict(X_cc_final, weights)                       \n",
        "Accu_eval(test_y10, Y_cc)\n",
        "print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn28oymB9pwD"
      },
      "source": [
        "**Confusion Matrix**\n",
        "- To run it please run one of the Logistic Regression algorithm for a particular case and then choose the argumments in the last line accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwkfD3gd5YcJ"
      },
      "source": [
        "def Conf_mat(test_y,pred_y):  \n",
        "    m=np.zeros((len(test_y,),2)).astype(int);                                     #Initializing Matrix 'm' with zeros that stores the actual target and predicted target column wise\n",
        "    m[:,0]=np.reshape(test_y,(len(test_y,)));                                     #Storing the actual target vector into the 1st column\n",
        "    m[:,1]=np.reshape(pred_y,(len(pred_y,)));                                     #Storing the predicted target vector into the 2nd column\n",
        "    \n",
        "    column_values = ['Original_y', 'Predicted_y']                                 # Creating the list containing column names\n",
        "    df = pd.DataFrame(data = m,columns = column_values)                           # Dataframe creation\n",
        "    df = df.to_string(index=False)\n",
        "\n",
        "    #Confusion Matrix : Class 1 - positive; Class 0 - negative; Actual Target Variable (Rows), Predicted Target Variable (Columns)\n",
        "    conf_mat = pd.crosstab(m[:,0],m[:,1],colnames=['Predicted Target Variable'],      \n",
        "                           rownames=['Actual Target Variable'], margins = False)      #Crosstab() creates a combination of rows and columns with 0 and 1, thus forming a table\n",
        "    sns.heatmap(conf_mat,annot=True,cmap='Blues',fmt='d')                             #Heatmap displays the confusion matrix\n",
        "    plt.title('Confusion Matrix');plt.show()\n",
        "    accuracy=(conf_mat.iat[0,0]+conf_mat.iat[1,1])/conf_mat.to_numpy().sum()          #Accuracy : (TP+TN) / (TP+FP+FN+TN);   \n",
        "    precision= conf_mat.iat[1,1]/(conf_mat.iat[1,1]+conf_mat.iat[0,1])                #Precision : TP / (TP+FP)\n",
        "    recall= conf_mat.iat[1,1]/(conf_mat.iat[1,1]+conf_mat.iat[1,0])                   #Recall : = TP / (TP+FN)\n",
        "    spec= conf_mat.iat[0,0]/(conf_mat.iat[0,0]+conf_mat.iat[0,1])                     #Specificity : TN / (FP+TN)\n",
        "    false_pos_rate = conf_mat.iat[0,1]/(conf_mat.iat[0,0]+conf_mat.iat[0,1])          #False Positive Rate : FP / (FP+TN)\n",
        "    print(\"Accuracy:%.2f\"%(accuracy*100),\"%\",\"\\nError:%.2f\"%(100-accuracy*100),\"%\")\n",
        "    print(\"Precision:%.2f\"%(precision*100),\"%\",\"\\nRecall:%.2f\"%(recall*100),\"%\",\"\\nSpecificity:%.2f\"%(spec*100),\"%\")\n",
        "    print(\"False Positive Rate:%.2f\"%(false_pos_rate*100),\"%\",\"\\nFalse Negative Rate:%.2f\"%(100-recall*100),\"%\")\n",
        "    print(\"F1 measure:%.2f\"%(2*((precision*recall)/(precision+recall)))); \n",
        "\n",
        "Conf_mat(test_y2, Y_cc)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6NwoREy9vSM"
      },
      "source": [
        "**Logistic Regression using inbuilt Python SciKit (sklearn) libraries**\n",
        "- For the Orthopedic Patinet the part is commented, you can uncomment it and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lkHT4j55ddb"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "model = LogisticRegression()\n",
        "model.fit(X_cc_T2, Y_cc.ravel())\n",
        "p_pred = model.predict_proba(X_cc_T2); \n",
        "y_pred = model.predict(X_cc_T2); \n",
        "score_ = model.score(X_cc_T2, Y_cc); \n",
        "conf_m = confusion_matrix(Y_cc, y_pred)\n",
        "report = classification_report(Y_cc, y_pred)\n",
        "# rescaledX_op = Standardize(X_op)\n",
        "# model.fit(rescaledX_op, Y_op.ravel())\n",
        "# p_pred = model.predict_proba(rescaledX_op); \n",
        "# y_pred = model.predict(rescaledX_op); \n",
        "# score_ = model.score(rescaledX_op, Y_op); \n",
        "# conf_m = confusion_matrix(Y_op, y_pred)\n",
        "# report = classification_report(Y_op, y_pred)\n",
        "print(\"The weights are :\\n\",model.coef_)                                        #weights\n",
        "print(\"\\nAccuracy Score is: \",score_)                                           #accuracy score\n",
        "print(\"\\nConfusion matrix is\\n\",conf_m)                                         #Confusion matrix\n",
        "print(\"\\nClassification Report is\\n\",report)                                    #Classification report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asfhx-lu93w4"
      },
      "source": [
        "**Naive Bayes Classifier using inbuilt Python SciKit (sklearn) libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5txaVfe5ecK"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "\n",
        "naive_bayes = GaussianNB()\n",
        "# rescaledX_op = Standardize(X_op)\n",
        "# naive_bayes.fit(normalizedX_op, Y_op.ravel())\n",
        "# y_prd = naive_bayes.predict(normalizedX_op)\n",
        "naive_bayes.fit(X_cc_T2, Y_cc.ravel())\n",
        "y_prd = naive_bayes.predict(X_cc_T2)\n",
        "print(\"Accuracy Score:\",metrics.accuracy_score(Y_cc, y_prd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22zrjTuovSt"
      },
      "source": [
        " **Scatter Plots for both the datasets**\n",
        " \n",
        " - Please make the neccessary change in the 7th line of the cell. \n",
        " - Results are also shown in the Appendix of the Report. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "php1902Xlx-v"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "\n",
        "sns.set_theme(style=\"ticks\")\n",
        "# sns_plot = sns.pairplot(df_op,hue='Class',diag_kind='hist',height=2.0)\n",
        "sns_plot = sns.pairplot(df_cc,hue='Class',diag_kind='hist',height=2.0)          #Rename the Target column as 'Class' in creditcard.csv file before execution.\n",
        "sns_plot.savefig(\"pairplot.png\")\n",
        "plt.clf() # Clean parirplot figure from sns \n",
        "Image(filename='pairplot.png') # Show pairplot as image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PEGpRADphPZ"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "  **Code Ends**\n",
        "\n",
        "---\n",
        "---"
      ]
    }
  ]
}